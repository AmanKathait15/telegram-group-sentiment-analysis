{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Importing all necessary python Modules ######\n",
    "\n",
    "import joblib , pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn import metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    text  Sentiment\n",
      "0                                missed new moon trailer          0\n",
      "1      omgaga sooo gunna cry dentist since suposed ge...          0\n",
      "2                                think mi bf cheating tt          0\n",
      "3                       juuuuuuuuuuuuuuuuussssst chillin          1\n",
      "4                         sunny work tomorrow tv tonight          0\n",
      "...                                                  ...        ...\n",
      "74890  foot really bad like worst ever barely walk right          0\n",
      "74891     fun health amp safety switch look spritely xxx          1\n",
      "74892  took waaay long get message ashamed right real...          0\n",
      "74893  seems like repeating problem hope able find so...          0\n",
      "74894  arrrr replied different tweets time see duno h...          1\n",
      "\n",
      "[74895 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "##### Reading Dataset ####\n",
    "\n",
    "df = pd.read_csv('dataset/clean_data.csv',encoding='latin-1')\n",
    "\n",
    "print(df)\n",
    "\n",
    "words = df['text'][0:1000]\n",
    "\n",
    "y = df['Sentiment'][0:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('zzzzzzzzzzzzz', 3379), ('zoro', 3378), ('zooooooooom', 3377), ('zone', 3376), ('zoffitcha', 3375), ('ziggy', 3374), ('zazzle', 3373), ('zahra', 3372), ('yw', 3371), ('yuuutsu', 3370), ('yummy', 3369), ('yum', 3368), ('yuck', 3367), ('yrs', 3366), ('yr', 3365), ('youu', 3364), ('youtube', 3363), ('youre', 3362), ('young', 3361), ('yoru', 3360)]\n",
      "\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "###### Bag of Word Model ######\n",
    "\n",
    "vect = CountVectorizer()\n",
    "\n",
    "vect.fit(words)\n",
    "\n",
    "tmp = list(vect.vocabulary_.items())\n",
    "\n",
    "tmp.sort(reverse = True)\n",
    "\n",
    "print(tmp[0:20])\n",
    "\n",
    "bgw = vect.transform(words)\n",
    "\n",
    "bgw = bgw.toarray()\n",
    "\n",
    "print('',bgw,sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "\n",
      "105    1\n",
      "68     0\n",
      "479    0\n",
      "399    0\n",
      "434    1\n",
      "      ..\n",
      "835    1\n",
      "192    0\n",
      "629    0\n",
      "559    0\n",
      "684    1\n",
      "Name: Sentiment, Length: 700, dtype: int64\n",
      "MultinomialNB() KNeighborsClassifier() RandomForestClassifier() LogisticRegression()\n"
     ]
    }
   ],
   "source": [
    "# splitting X and y into training and testing sets \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(bgw, y, test_size=0.3, random_state=0)\n",
    "\n",
    "print(X_train ,'', y_train , sep='\\n')\n",
    "\n",
    "mnb = MultinomialNB() \n",
    "\n",
    "knc = KNeighborsClassifier()\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "lr = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB() KNeighborsClassifier() RandomForestClassifier() LogisticRegression()\n",
      "[0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 1 1 1 0 0 0 0\n",
      " 1 0 0 0 0 0 0 1 0 0 0 1 1 0 0 1 1 0 0 1 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 1\n",
      " 0 0 0 0 1 1 0 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 1 0 1 0 0 1 0 0 0 1 1 0 0 0 0 1 1 0 0 1\n",
      " 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 1 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0\n",
      " 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 1 0 1 0 1 0 0 1 1 0 1 0 0\n",
      " 0 0 0 0 1 1 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0 0 0 1 1 0 0 0 0 1 0 0 1 1 0 0 0\n",
      " 0 1 0 1 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 1 1 0 0 1 1 0 0 1 1 0 0 0 1 0 0 0\n",
      " 0 0 0 0] [0 1 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 1 1 0 0 0 0 0 0 1 1 0 0 1 1 0\n",
      " 1 1 1 0 0 0 1 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 1 1 1 0 0 1 0 0 0 1 1 1 0 1\n",
      " 0 0 1 0 1 1 0 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0\n",
      " 1 1 1 0 0 1 0 0 1 0 0 1 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 1 1 0 0 0 1\n",
      " 0 1 0 0 1 0 0 0 0 0 0 0 1 0 1 1 0 1 1 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 1\n",
      " 0 1 0 0 1 1 0 0 0 0 1 0 1 0 0 0 0 0 1 1 0 0 0 1 0 1 0 0 0 0 0 1 0 0 1 1 0\n",
      " 0 0 1 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0 1 0 0 0 0 0 1 1 1 1 1 1 1 0 1 1 0 0 1\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 1 1 0 0 0 1\n",
      " 0 0 0 1] [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 1 1 0 0 0\n",
      " 0 0 0 0] [0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0\n",
      " 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 1 1 0 1 0 0\n",
      " 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0 1 0 0 0 1\n",
      " 0 0 0 1 1 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 1 0 0 0 1 1 0 0 0 1 0 0 1 0 0 0 0\n",
      " 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "knc.fit(X_train, y_train)\n",
    "\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "mnb.fit(X_train, y_train) \n",
    "\n",
    "print(mnb,knc,rf,lr)\n",
    "\n",
    "y_pred1 = mnb.predict(X_test) \n",
    "\n",
    "y_pred2 = knc.predict(X_test)\n",
    "\n",
    "y_pred3 = rf.predict(X_test)\n",
    "\n",
    "y_pred4 = lr.predict(X_test) \n",
    "\n",
    "print(y_pred1,y_pred2,y_pred3,y_pred4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB model accuracy(in %): 46.666666666666664\n",
      "KNeighborsClassifier model accuracy(in %): 76.66666666666667\n",
      "RandomForestClassifier model accuracy(in %): 76.66666666666667\n",
      "LogisticRegression model accuracy(in %): 76.66666666666667\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# comparing actual response values (y_test) with predicted response values (y_pred) \n",
    "\n",
    "print(\"MultinomialNB model accuracy(in %):\", metrics.accuracy_score(y_test, y_pred1)*100)\n",
    "\n",
    "print(\"KNeighborsClassifier model accuracy(in %):\", metrics.accuracy_score(y_test, y_pred2)*100)\n",
    "\n",
    "print(\"RandomForestClassifier model accuracy(in %):\", metrics.accuracy_score(y_test, y_pred3)*100)\n",
    "\n",
    "print(\"LogisticRegression model accuracy(in %):\", metrics.accuracy_score(y_test, y_pred4)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model/LogisticRegression_bgw.pkl']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the model as a pickle in a file \n",
    "\n",
    "joblib.dump(mnb, 'model/MultinomialNB_bgw.pkl') \n",
    "\n",
    "joblib.dump(knc, 'model/KNeighborsClassifier_bgw.pkl') \n",
    "\n",
    "joblib.dump(rf, 'model/RandomForestClassifier_bgw.pkl') \n",
    "\n",
    "joblib.dump(lr, 'model/LogisticRegression_bgw.pkl') "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
