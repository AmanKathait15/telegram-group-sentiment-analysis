{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Importing all necessary python Modules ######\n",
    "\n",
    "import csv,operator,re,itertools\n",
    "import pandas as pd\n",
    "\n",
    "from string import punctuation\n",
    "from collections import Counter\n",
    "\n",
    "from nltk.corpus import stopwords  \n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_smileys = {\n",
    "        \":‑)\":\"smiley\",\n",
    "        \":-]\":\"smiley\",\n",
    "        \":-3\":\"smiley\",\n",
    "        \":->\":\"smiley\",\n",
    "        \"8-)\":\"smiley\",\n",
    "        \":-}\":\"smiley\",\n",
    "        \":)\":\"smiley\",\n",
    "        \":]\":\"smiley\",\n",
    "        \":3\":\"smiley\",\n",
    "        \":>\":\"smiley\",\n",
    "        \"8)\":\"smiley\",\n",
    "        \":}\":\"smiley\",\n",
    "        \":o)\":\"smiley\",\n",
    "        \":c)\":\"smiley\",\n",
    "        \":^)\":\"smiley\",\n",
    "        \"=]\":\"smiley\",\n",
    "        \"=)\":\"smiley\",\n",
    "        \":-))\":\"smiley\",\n",
    "        \":‑D\":\"smiley\",\n",
    "        \"8‑D\":\"smiley\",\n",
    "        \"x‑D\":\"smiley\",\n",
    "        \"X‑D\":\"smiley\",\n",
    "        \":D\":\"smiley\",\n",
    "        \"8D\":\"smiley\",\n",
    "        \"xD\":\"smiley\",\n",
    "        \"XD\":\"smiley\",\n",
    "        \":‑(\":\"sad\",\n",
    "        \":‑c\":\"sad\",\n",
    "        \":‑<\":\"sad\",\n",
    "        \":‑[\":\"sad\",\n",
    "        \":(\":\"sad\",\n",
    "        \":c\":\"sad\",\n",
    "        \":<\":\"sad\",\n",
    "        \":[\":\"sad\",\n",
    "        \":-||\":\"sad\",\n",
    "        \">:[\":\"sad\",\n",
    "        \":{\":\"sad\",\n",
    "        \":@\":\"sad\",\n",
    "        \">:(\":\"sad\",\n",
    "        \":'‑(\":\"sad\",\n",
    "        \":'(\":\"sad\",\n",
    "        \":‑P\":\"playful\",\n",
    "        \"X‑P\":\"playful\",\n",
    "        \"x‑p\":\"playful\",\n",
    "        \":‑p\":\"playful\",\n",
    "        \":‑Þ\":\"playful\",\n",
    "        \":‑þ\":\"playful\",\n",
    "        \":‑b\":\"playful\",\n",
    "        \":P\":\"playful\",\n",
    "        \"XP\":\"playful\",\n",
    "        \"xp\":\"playful\",\n",
    "        \":p\":\"playful\",\n",
    "        \":Þ\":\"playful\",\n",
    "        \":þ\":\"playful\",\n",
    "        \":b\":\"playful\",\n",
    "        \"<3\":\"love\"\n",
    "        }\n",
    "\n",
    "dict_contractions = {\n",
    "            \"ain't\":\"is not\",\n",
    "            \"amn't\":\"am not\",\n",
    "            \"aren't\":\"are not\",\n",
    "            \"can't\":\"can not\",\n",
    "            \"'cause\":\"because\",\n",
    "            \"couldn't\":\"could not\",\n",
    "            \"couldn't've\":\"could not have\",\n",
    "            \"could've\":\"could have\",\n",
    "            \"daren't\":\"dare not\",\n",
    "            \"daresn't\":\"dare not\",\n",
    "            \"dasn't\":\"dare not\",\n",
    "            \"didn't\":\"did not\",\n",
    "            \"doesn't\":\"does not\",\n",
    "            \"don't\":\"do not\",\n",
    "            \"e'er\":\"ever\",\n",
    "            \"em\":\"them\",\n",
    "            \"everyone's\":\"everyone is\",\n",
    "            \"finna\":\"fixing to\",\n",
    "            \"gimme\":\"give me\",\n",
    "            \"gonna\":\"going to\",\n",
    "            \"gon't\":\"go not\",\n",
    "            \"gotta\":\"got to\",\n",
    "            \"hadn't\":\"had not\",\n",
    "            \"hasn't\":\"has not\",\n",
    "            \"haven't\":\"have not\",\n",
    "            \"he'd\":\"he would\",\n",
    "            \"he'll\":\"he will\",\n",
    "            \"he's\":\"he is\",\n",
    "            \"he've\":\"he have\",\n",
    "            \"how'd\":\"how would\",\n",
    "            \"how'll\":\"how will\",\n",
    "            \"how're\":\"how are\",\n",
    "            \"how's\":\"how is\",\n",
    "            \"i'd\":\"i would\",\n",
    "            \"i'll\":\"i will\",\n",
    "            \"i'm\":\"i am\",\n",
    "            \"im\":\"i am\",\n",
    "            \"i'm'a\":\"i am about to\",\n",
    "            \"i'm'o\":\"i am going to\",\n",
    "            \"isn't\":\"is not\",\n",
    "            \"it'd\":\"it would\",\n",
    "            \"it'll\":\"it will\",\n",
    "            \"it's\":\"it is\",\n",
    "            \"i've\":\"i have\",\n",
    "            \"kinda\":\"kind of\",\n",
    "            \"let's\":\"let us\",\n",
    "            \"mayn't\":\"may not\",\n",
    "            \"may've\":\"may have\",\n",
    "            \"mightn't\":\"might not\",\n",
    "            \"might've\":\"might have\",\n",
    "            \"mustn't\":\"must not\",\n",
    "            \"mustn't've\":\"must not have\",\n",
    "            \"must've\":\"must have\",\n",
    "            \"needn't\":\"need not\",\n",
    "            \"ne'er\":\"never\",\n",
    "            \"o'\":\"of\",\n",
    "            \"o'er\":\"over\",\n",
    "            \"ol'\":\"old\",\n",
    "            \"oughtn't\":\"ought not\",\n",
    "            \"shalln't\":\"shall not\",\n",
    "            \"shan't\":\"shall not\",\n",
    "            \"she'd\":\"she would\",\n",
    "            \"she'll\":\"she will\",\n",
    "            \"she's\":\"she is\",\n",
    "            \"shouldn't\":\"should not\",\n",
    "            \"shouldn't've\":\"should not have\",\n",
    "            \"should've\":\"should have\",\n",
    "            \"somebody's\":\"somebody is\",\n",
    "            \"someone's\":\"someone is\",\n",
    "            \"something's\":\"something is\",\n",
    "            \"that'd\":\"that would\",\n",
    "            \"that'll\":\"that will\",\n",
    "            \"that're\":\"that are\",\n",
    "            \"that's\":\"that is\",\n",
    "            \"there'd\":\"there would\",\n",
    "            \"there'll\":\"there will\",\n",
    "            \"there're\":\"there are\",\n",
    "            \"there's\":\"there is\",\n",
    "            \"these're\":\"these are\",\n",
    "            \"they'd\":\"they would\",\n",
    "            \"they'll\":\"they will\",\n",
    "            \"they're\":\"they are\",\n",
    "            \"they've\":\"they have\",\n",
    "            \"this's\":\"this is\",\n",
    "            \"those're\":\"those are\",\n",
    "            \"'tis\":\"it is\",\n",
    "            \"'twas\":\"it was\",\n",
    "            \"wanna\":\"want to\",\n",
    "            \"wasn't\":\"was not\",\n",
    "            \"we'd\":\"we would\",\n",
    "            \"we'd've\":\"we would have\",\n",
    "            \"we'll\":\"we will\",\n",
    "            \"we're\":\"we are\",\n",
    "            \"weren't\":\"were not\",\n",
    "            \"we've\":\"we have\",\n",
    "            \"what'd\":\"what did\",\n",
    "            \"what'll\":\"what will\",\n",
    "            \"what're\":\"what are\",\n",
    "            \"what's\":\"what is\",\n",
    "            \"what've\":\"what have\",\n",
    "            \"when's\":\"when is\",\n",
    "            \"where'd\":\"where did\",\n",
    "            \"where're\":\"where are\",\n",
    "            \"where's\":\"where is\",\n",
    "            \"where've\":\"where have\",\n",
    "            \"which's\":\"which is\",\n",
    "            \"who'd\":\"who would\",\n",
    "            \"who'd've\":\"who would have\",\n",
    "            \"who'll\":\"who will\",\n",
    "            \"who're\":\"who are\",\n",
    "            \"who's\":\"who is\",\n",
    "            \"who've\":\"who have\",\n",
    "            \"why'd\":\"why did\",\n",
    "            \"why're\":\"why are\",\n",
    "            \"why's\":\"why is\",\n",
    "            \"won't\":\"will not\",\n",
    "            \"wouldn't\":\"would not\",\n",
    "            \"would've\":\"would have\",\n",
    "            \"y'all\":\"you all\",\n",
    "            \"you'd\":\"you would\",\n",
    "            \"you'll\":\"you will\",\n",
    "            \"you're\":\"you are\",\n",
    "            \"you've\":\"you have\",\n",
    "            \"Whatcha\":\"What are you\",\n",
    "            \"luv\":\"love\",\n",
    "            \"sux\":\"sucks\"\n",
    "            }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " the AC is on!!\n",
      "ac\n"
     ]
    }
   ],
   "source": [
    "def clean_data(text):    \n",
    "    \n",
    "    #Escaping HTML characters\n",
    "\n",
    "    text = text.lower()\n",
    "\n",
    "    text = text.replace(\"’\",\"'\")\n",
    "\n",
    "    tokens = text.split()\n",
    "    \n",
    "    tokens = [dict_contractions[word] if word in dict_contractions else word for word in tokens]\n",
    "\n",
    "    tokens = [dict_smileys[word] if word in dict_smileys else word for word in tokens]\n",
    "\n",
    "    text = ' '.join(tokens)\n",
    "\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\n",
    "   \n",
    "    #Special case not handled previously.\n",
    "    text = text.replace('\\x92',\"'\")\n",
    "\n",
    "    #removing retweet tag\n",
    "    text = text.replace('RT',\"\")\n",
    "    \n",
    "    #Removal of hastags\n",
    "    text = re.sub(r\"#[A-Za-z0-9]+\", \"\", text)\n",
    "\n",
    "    #Removal of handle\n",
    "    text = re.sub(r\"@[A-Za-z0-9_]+\", \"\", text,5)\n",
    "    \n",
    "    #Removal of address\n",
    "    text = re.sub(\"(\\w+:\\/\\/\\S+)\", \"\", text)\n",
    "\n",
    "    #Removal of Punctuation\n",
    "    text = re.sub(\"[\\.\\,\\!\\?\\:\\;\\-\\=]\", \" \", text)\n",
    "\n",
    "    text = re.sub(\"[^A-Za-z ]\", \"\", text)\n",
    "\n",
    "    tokens = text.split()\n",
    "    \n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    \n",
    "    # filter out stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "\n",
    "    # filter out short tokens\n",
    "    tokens = [word for word in tokens if len(word) > 1]\n",
    "\n",
    "    text = ' '.join(tokens)\n",
    "\n",
    "    #print(text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "from random import randint\n",
    "\n",
    "df = pd.read_csv('dataset/train_data.csv',encoding='latin-1')\n",
    "\n",
    "words = df['text']\n",
    "\n",
    "i = randint(0,10000)\n",
    "\n",
    "print(words[i],clean_data(words[i]),sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       ItemID  Sentiment                                               text\n",
      "0           1          0                       is so sad for my APL frie...\n",
      "1           2          0                     I missed the New Moon trail...\n",
      "2           3          1                            omg its already 7:30 :O\n",
      "3           4          0            .. Omgaga. Im sooo  im gunna CRy. I'...\n",
      "4           5          0           i think mi bf is cheating on me!!!   ...\n",
      "...       ...        ...                                                ...\n",
      "99984   99996          0  @Cupcake  seems like a repeating problem   hop...\n",
      "99985   99997          1  @cupcake__ arrrr we both replied to each other...\n",
      "99986   99998          0                     @CuPcAkE_2120 ya i thought so \n",
      "99987   99999          1  @Cupcake_Dollie Yes. Yes. I'm glad you had mor...\n",
      "99988  100000          1                    @cupcake_kayla haha yes you do \n",
      "\n",
      "[99989 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('dataset/train_data.csv',encoding='latin-1')\n",
    "\n",
    "print(df)\n",
    "\n",
    "words = df['text']\n",
    "\n",
    "y = df['Sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    text  Sentiment\n",
      "0                                missed new moon trailer          0\n",
      "1      omgaga sooo gunna cry dentist since suposed ge...          0\n",
      "2                                think mi bf cheating tt          0\n",
      "3                       juuuuuuuuuuuuuuuuussssst chillin          1\n",
      "4                         sunny work tomorrow tv tonight          0\n",
      "...                                                  ...        ...\n",
      "74857                            prada linea rossa shoes          1\n",
      "74858                   hey love delores park sf anymore          0\n",
      "74859  well try carolyn although would lying said fin...          1\n",
      "74860  ok thanks let know think going go play box men...          1\n",
      "74861  brittish parliament challenges clearly puppete...          1\n",
      "\n",
      "[74862 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "fp = open('dataset/clean_data.csv','w')\n",
    "\n",
    "fp.write('text,Sentiment')\n",
    "\n",
    "for i in range(0,len(words)):\n",
    "\n",
    "    w = clean_data(words[i])\n",
    "\n",
    "    if(len(w)<20):\n",
    "\n",
    "        continue\n",
    "\n",
    "    fp.write('\\n'+w+','+str(y[i]))\n",
    "    \n",
    "df = pd.read_csv('dataset/clean_data.csv')\n",
    "\n",
    "print(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
